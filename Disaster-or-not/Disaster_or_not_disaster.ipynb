{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Disaster_or_not_disaster.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "24LzgteKY-kR",
        "K-GxjeslZRQM",
        "-y2XaXMtdN4j",
        "sFk9Nc7LdTpn",
        "wXSd4xo7jX63",
        "RdXIAiZ-oARo",
        "ynZ0_3AgjkmE",
        "iiQsLbo8yLsx"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D5QLYr2YV60E",
        "outputId": "7887158f-91aa-4f87-94c1-3cd75c426f63"
      },
      "source": [
        "#Check GPU\n",
        "!nvidia-smi -L"
      ],
      "execution_count": 216,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU 0: Tesla K80 (UUID: GPU-9e58ec82-e194-c014-5e52-ce1aa343e7eb)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "24LzgteKY-kR"
      },
      "source": [
        "#Add data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yP-k0u3UWSgH",
        "outputId": "02a6f167-ef81-46cc-d03c-f371c1ee7f60"
      },
      "source": [
        "import zipfile\n",
        "# Download data (same as from Kaggle)\n",
        "!wget \"https://storage.googleapis.com/ztm_tf_course/nlp_getting_started.zip\"\n",
        "\n",
        "# Unzip data\n",
        "\n",
        "zip_ref = zipfile.ZipFile(\"nlp_getting_started.zip\", \"r\")\n",
        "zip_ref.extractall()\n",
        "zip_ref.close()"
      ],
      "execution_count": 217,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2021-10-28 09:05:39--  https://storage.googleapis.com/ztm_tf_course/nlp_getting_started.zip\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 64.233.189.128, 108.177.97.128, 108.177.125.128, ...\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|64.233.189.128|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 607343 (593K) [application/zip]\n",
            "Saving to: ‘nlp_getting_started.zip.4’\n",
            "\n",
            "\r          nlp_getti   0%[                    ]       0  --.-KB/s               \rnlp_getting_started 100%[===================>] 593.11K  --.-KB/s    in 0.006s  \n",
            "\n",
            "2021-10-28 09:05:39 (97.9 MB/s) - ‘nlp_getting_started.zip.4’ saved [607343/607343]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K-GxjeslZRQM"
      },
      "source": [
        "# EDA"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0VhKrurWbo-X",
        "outputId": "0283c8e5-8675-49ef-c925-e9b9a77385ec"
      },
      "source": [
        "pd.set_option(\"display.max_colwidth\", -1)\n",
        "pd.set_option(\"display.max_rows\", None)\n",
        "pd.set_option(\"display.max_columns\", None)"
      ],
      "execution_count": 218,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:1: FutureWarning: Passing a negative integer is deprecated in version 1.0 and will not be supported in future version. Instead, use None to not limit the column width.\n",
            "  \"\"\"Entry point for launching an IPython kernel.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BMvf-kJmXV49"
      },
      "source": [
        "import pandas as pd\n",
        "train_data = pd.read_csv('train.csv')\n",
        "test_data = pd.read_csv('test.csv')"
      ],
      "execution_count": 219,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WkpZm6DuXmNU",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 270
        },
        "outputId": "3e82bc6b-0e43-4840-8bcb-7406c5e2f571"
      },
      "source": [
        "print(train_data.shape ,test_data.shape)\n",
        "train_data.sample(frac=1,random_state=42).head()"
      ],
      "execution_count": 220,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(7613, 5) (3263, 4)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>keyword</th>\n",
              "      <th>location</th>\n",
              "      <th>text</th>\n",
              "      <th>target</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2644</th>\n",
              "      <td>3796</td>\n",
              "      <td>destruction</td>\n",
              "      <td>NaN</td>\n",
              "      <td>So you have a new weapon that can cause un-imaginable destruction.</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2227</th>\n",
              "      <td>3185</td>\n",
              "      <td>deluge</td>\n",
              "      <td>NaN</td>\n",
              "      <td>The f$&amp;amp;@ing things I do for #GISHWHES Just got soaked in a deluge going for pads and tampons. Thx @mishacollins @/@</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5448</th>\n",
              "      <td>7769</td>\n",
              "      <td>police</td>\n",
              "      <td>UK</td>\n",
              "      <td>DT @georgegalloway: RT @Galloway4Mayor: ÛÏThe CoL police can catch a pickpocket in Liverpool Stree... http://t.co/vXIn1gOq4Q</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>132</th>\n",
              "      <td>191</td>\n",
              "      <td>aftershock</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Aftershock back to school kick off was great. I want to thank everyone for making it possible. What a great night.</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6845</th>\n",
              "      <td>9810</td>\n",
              "      <td>trauma</td>\n",
              "      <td>Montgomery County, MD</td>\n",
              "      <td>in response to trauma Children of Addicts develop a defensive self - one that decreases vulnerability. (3</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "        id      keyword               location  \\\n",
              "2644  3796  destruction  NaN                     \n",
              "2227  3185  deluge       NaN                     \n",
              "5448  7769  police       UK                      \n",
              "132   191   aftershock   NaN                     \n",
              "6845  9810  trauma       Montgomery County, MD   \n",
              "\n",
              "                                                                                                                               text  \\\n",
              "2644  So you have a new weapon that can cause un-imaginable destruction.                                                              \n",
              "2227  The f$&amp;@ing things I do for #GISHWHES Just got soaked in a deluge going for pads and tampons. Thx @mishacollins @/@         \n",
              "5448  DT @georgegalloway: RT @Galloway4Mayor: ÛÏThe CoL police can catch a pickpocket in Liverpool Stree... http://t.co/vXIn1gOq4Q   \n",
              "132   Aftershock back to school kick off was great. I want to thank everyone for making it possible. What a great night.              \n",
              "6845  in response to trauma Children of Addicts develop a defensive self - one that decreases vulnerability. (3                       \n",
              "\n",
              "      target  \n",
              "2644  1       \n",
              "2227  0       \n",
              "5448  1       \n",
              "132   0       \n",
              "6845  0       "
            ]
          },
          "metadata": {},
          "execution_count": 220
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "APd8wuM3XpKc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "79d5f5a7-af8a-4a57-b827-b4f64d89a5c1"
      },
      "source": [
        "train_data.info()"
      ],
      "execution_count": 221,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 7613 entries, 0 to 7612\n",
            "Data columns (total 5 columns):\n",
            " #   Column    Non-Null Count  Dtype \n",
            "---  ------    --------------  ----- \n",
            " 0   id        7613 non-null   int64 \n",
            " 1   keyword   7552 non-null   object\n",
            " 2   location  5080 non-null   object\n",
            " 3   text      7613 non-null   object\n",
            " 4   target    7613 non-null   int64 \n",
            "dtypes: int64(2), object(3)\n",
            "memory usage: 297.5+ KB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ga2J3ICsZjeA"
      },
      "source": [
        " WE have 2 features that have NAN-values \n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eAJ53Bt2aIe6"
      },
      "source": [
        "### Count plot"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sFrrZIOPYWtr",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "outputId": "3b9f05ca-5309-4f8d-f075-53b2206ce870"
      },
      "source": [
        "# Check our target is balenced or not!\n",
        "train_data.target.value_counts().sort_values().plot(kind = 'bar')"
      ],
      "execution_count": 222,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7fadf29dce50>"
            ]
          },
          "metadata": {},
          "execution_count": 222
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-y2XaXMtdN4j"
      },
      "source": [
        "### Visit some random data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Spv3sGmpaPPC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2cd80ecd-0686-4bf2-ccd4-17f9f9799eb5"
      },
      "source": [
        "import random\n",
        "randomIndex = random.randint(0, len(train_data)-5) \n",
        "for row in train_data[[\"text\", \"target\"]][randomIndex:randomIndex+5].itertuples():\n",
        "  _, text, target = row\n",
        "  print(f\"Target: {target}\", \"(real disaster)\" if target > 0 else \"(not real disaster)\")\n",
        "  print(f\"Text:\\n{text}\\n\")\n",
        "  print(\"---\\n\")"
      ],
      "execution_count": 223,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Target: 0 (not real disaster)\n",
            "Text:\n",
            "@IAN_Hellfire I got it for the mistake but boss got it worse cause their job was to oversee my work. Boss didn't change after that...\n",
            "\n",
            "---\n",
            "\n",
            "Target: 0 (not real disaster)\n",
            "Text:\n",
            "That hellfire song from the hunchback of notre dame reminds me a lot of my house\n",
            "\n",
            "---\n",
            "\n",
            "Target: 0 (not real disaster)\n",
            "Text:\n",
            "the message you sent and they don't reply. I see that you saw my message the least you can do is tell me to 'fuck off' or something.\n",
            "\n",
            "---\n",
            "\n",
            "Target: 0 (not real disaster)\n",
            "Text:\n",
            "Beware of your temper and a loose tongue! These two dangerous weapons combined can lead a person to the Hellfire #islam!\n",
            "\n",
            "---\n",
            "\n",
            "Target: 0 (not real disaster)\n",
            "Text:\n",
            "The Prophet (peace be upon him) said 'Save yourself from Hellfire even if it is by giving half a date in charity.'\n",
            "\n",
            "---\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sFk9Nc7LdTpn"
      },
      "source": [
        "# Research for modeling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UZ9RKCg9dXhs"
      },
      "source": [
        "### split data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7KcgAmiDcSXg"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "xTrain, xVal, yTrain, yVal = train_test_split(train_data[\"text\"].to_numpy(),\n",
        "                                                                            train_data[\"target\"].to_numpy(),\n",
        "                                                                            test_size=0.15,\n",
        "                                                                            random_state=55)\n",
        "xTest = pd.read_csv('test.csv')\n",
        "id = xTest['id']"
      ],
      "execution_count": 224,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "toEDBB6GhZ2S"
      },
      "source": [
        "### Text Vectorization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "as8LYp1sdy6W"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n",
        "\n",
        "textVector = TextVectorization(max_tokens=None,\n",
        "                                    standardize=\"lower_and_strip_punctuation\", \n",
        "                                    split=\"whitespace\",\n",
        "                                    ngrams=None, \n",
        "                                    output_mode=\"int\",\n",
        "                                    output_sequence_length=None) \n"
      ],
      "execution_count": 225,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iPb6UQDod47V"
      },
      "source": [
        "# Find average number of tokens (words) in training Tweets\n",
        "avrageWord = round(sum([len(i.split()) for i in xTrain])/len(xTrain))"
      ],
      "execution_count": 226,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gpz3e9zghUqL"
      },
      "source": [
        "textVector.adapt(xTrain)"
      ],
      "execution_count": 227,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pT0KQ5fhd3yL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "50a6f22b-e23b-4fc7-c6ec-a7ae1cf3885e"
      },
      "source": [
        "textVector(['This is my first sentence'])"
      ],
      "execution_count": 228,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(1, 5), dtype=int64, numpy=array([[19,  9, 13, 96,  1]])>"
            ]
          },
          "metadata": {},
          "execution_count": 228
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kEpBfLMOeP-A",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a1470124-1cc1-43a2-b93c-6856a93f27f0"
      },
      "source": [
        "# The most used vocab in data\n",
        "textVector.get_vocabulary()[:20]"
      ],
      "execution_count": 312,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['',\n",
              " '[UNK]',\n",
              " 'the',\n",
              " 'a',\n",
              " 'in',\n",
              " 'to',\n",
              " 'of',\n",
              " 'and',\n",
              " 'i',\n",
              " 'is',\n",
              " 'for',\n",
              " 'on',\n",
              " 'you',\n",
              " 'my',\n",
              " 'it',\n",
              " 'that',\n",
              " 'with',\n",
              " 'at',\n",
              " 'by',\n",
              " 'this']"
            ]
          },
          "metadata": {},
          "execution_count": 312
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mh7CsLIth345"
      },
      "source": [
        "### Model 0 : Naive bayes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JW11ovgxe8DC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0def9b3e-39af-4e61-ee74-d54d983bf3af"
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "embedding = tf.keras.layers.Embedding(input_dim=1000,\n",
        "                             output_dim=128,\n",
        "                             embeddings_initializer=\"uniform\",\n",
        "                             input_length=avrageWord,\n",
        "                             name=\"embedding_1\")\n",
        "\n",
        "embedding"
      ],
      "execution_count": 230,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.layers.embeddings.Embedding at 0x7fadf246ca10>"
            ]
          },
          "metadata": {},
          "execution_count": 230
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "npw1e60FfoMN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7f52023f-1b83-4598-f49a-6ec46041d283"
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "# Create tokenization and modelling pipeline\n",
        "model0 = Pipeline([\n",
        "                    (\"tfidf\", TfidfVectorizer()), # convert words to numbers using tfidf\n",
        "                    (\"clf\", MultinomialNB()) # model the text\n",
        "])\n",
        "\n",
        "# Fit the pipeline to the training data\n",
        "model0.fit(xTrain, yTrain)\n",
        "\n"
      ],
      "execution_count": 231,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Pipeline(memory=None,\n",
              "         steps=[('tfidf',\n",
              "                 TfidfVectorizer(analyzer='word', binary=False,\n",
              "                                 decode_error='strict',\n",
              "                                 dtype=<class 'numpy.float64'>,\n",
              "                                 encoding='utf-8', input='content',\n",
              "                                 lowercase=True, max_df=1.0, max_features=None,\n",
              "                                 min_df=1, ngram_range=(1, 1), norm='l2',\n",
              "                                 preprocessor=None, smooth_idf=True,\n",
              "                                 stop_words=None, strip_accents=None,\n",
              "                                 sublinear_tf=False,\n",
              "                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
              "                                 tokenizer=None, use_idf=True,\n",
              "                                 vocabulary=None)),\n",
              "                ('clf',\n",
              "                 MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))],\n",
              "         verbose=False)"
            ]
          },
          "metadata": {},
          "execution_count": 231
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i1IcO1gCgosk"
      },
      "source": [
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
        "\n",
        "# Create a function for cal. score of our model ( will be used on other models)\n",
        "def CalculateResults(yTrue, yPred):\n",
        "\n",
        "  # Calculate model accuracy\n",
        "  modelAccuracy = accuracy_score(yTrue, yPred) * 100\n",
        "  # Calculate model precision, recall and f1 score using \"weighted\" average\n",
        "  modelPrecision, modelRecall, modelF1, _ = precision_recall_fscore_support(yTrue, yPred, average=\"weighted\")\n",
        "  modelResults = {\"accuracy\": modelAccuracy,\n",
        "                  \"precision\": modelPrecision,\n",
        "                  \"recall\": modelRecall,\n",
        "                  \"f1\": modelF1}\n",
        "  return modelResults"
      ],
      "execution_count": 232,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wXSd4xo7jX63"
      },
      "source": [
        "### Score model 0"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lnx3Y6WnisDC"
      },
      "source": [
        "yPred = model0.predict(xVal)\n",
        "resualt0 = CalculateResults(yVal,yPred)"
      ],
      "execution_count": 233,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RdXIAiZ-oARo"
      },
      "source": [
        "### Predict for kaggle score: 79.4"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1F6_zTYIn_q6"
      },
      "source": [
        "resualt = model0.predict(xTest['text'])\n",
        "resualt = tf.squeeze(tf.round(resualt))\n",
        "resaultDataframe = pd.DataFrame({'id':id,'target':resualt})\n",
        "resaultDataframe.to_csv('submission.csv',index=False)"
      ],
      "execution_count": 234,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ynZ0_3AgjkmE"
      },
      "source": [
        "### Model 1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TRRrjjTmjFqM"
      },
      "source": [
        "import datetime\n",
        "def CreateTensorboardCallback(dirName, experimentName):\n",
        "\n",
        "  logDir = dirName + \"/\" + experimentName + \"/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "  tensorboardCallback = tf.keras.callbacks.TensorBoard(\n",
        "      log_dir=logDir\n",
        "  )\n",
        "  print(f\"Saving TensorBoard log files to: {logDir}\")\n",
        "  return tensorboardCallback"
      ],
      "execution_count": 235,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WM04JO5pjjpv"
      },
      "source": [
        "SAVE_DIR = \"model_logs\""
      ],
      "execution_count": 236,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Duxs-nYzkvvi"
      },
      "source": [
        "from tensorflow.keras import layers\n",
        "inputs = layers.Input(shape=(1,), dtype=\"string\") # inputs are 1-dimensional strings\n",
        "x = textVector(inputs) # turn the input text into numbers\n",
        "x = embedding(x) \n",
        "x = layers.GlobalAveragePooling1D()(x)\n",
        "outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
        "model1 = tf.keras.Model(inputs, outputs, name=\"model_1_dense\") "
      ],
      "execution_count": 237,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M3rP6tcBld5P"
      },
      "source": [
        "# Compile model\n",
        "model1.compile(loss=\"binary_crossentropy\",\n",
        "                optimizer=tf.keras.optimizers.Adam(),\n",
        "                metrics=[\"accuracy\"])"
      ],
      "execution_count": 238,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RNuGd_lhleSA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1c2fbec6-4e17-4d0f-b85a-3eb2e1769f91"
      },
      "source": [
        "# Fit the model\n",
        "model1History = model1.fit(xTrain, # input sentences can be a list of strings due to text preprocessing layer built-in model\n",
        "                              yTrain,\n",
        "                              epochs=5,\n",
        "                              validation_data=(xVal, yVal),\n",
        "                              callbacks=[CreateTensorboardCallback(dirName=SAVE_DIR, \n",
        "                                                                     experimentName=\"simple_dense_model\")])"
      ],
      "execution_count": 239,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving TensorBoard log files to: model_logs/simple_dense_model/20211028-090540\n",
            "Epoch 1/5\n",
            "203/203 [==============================] - 3s 11ms/step - loss: 0.6467 - accuracy: 0.6151 - val_loss: 0.5857 - val_accuracy: 0.7548\n",
            "Epoch 2/5\n",
            "203/203 [==============================] - 2s 8ms/step - loss: 0.5329 - accuracy: 0.7719 - val_loss: 0.4953 - val_accuracy: 0.7723\n",
            "Epoch 3/5\n",
            "203/203 [==============================] - 2s 8ms/step - loss: 0.4646 - accuracy: 0.8051 - val_loss: 0.4667 - val_accuracy: 0.7837\n",
            "Epoch 4/5\n",
            "203/203 [==============================] - 2s 8ms/step - loss: 0.4302 - accuracy: 0.8159 - val_loss: 0.4573 - val_accuracy: 0.7855\n",
            "Epoch 5/5\n",
            "203/203 [==============================] - 2s 8ms/step - loss: 0.4077 - accuracy: 0.8285 - val_loss: 0.4575 - val_accuracy: 0.7872\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xhOcBvboltn5"
      },
      "source": [
        "yPred = model1.predict(xVal)\n",
        "yPred = tf.squeeze(tf.round(yPred))\n",
        "resualt1 = CalculateResults(yVal,yPred)\n"
      ],
      "execution_count": 240,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w5aAsx7V0XXf"
      },
      "source": [
        "### Visualizing embedding words"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AAGWzMUszrPB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fd97f8c8-d143-4d5c-e728-663f0559a6e3"
      },
      "source": [
        "wordsInVocab = textVector.get_vocabulary()\n",
        "embedWeights = model1.get_layer(\"embedding_1\").get_weights()[0]\n",
        "embedWeights.shape , len(wordsInVocab), wordsInVocab[:10]"
      ],
      "execution_count": 241,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((1000, 128),\n",
              " 20255,\n",
              " ['', '[UNK]', 'the', 'a', 'in', 'to', 'of', 'and', 'i', 'is'])"
            ]
          },
          "metadata": {},
          "execution_count": 241
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YL5oYedfmpe6"
      },
      "source": [
        "import io\n",
        "# Create output writers\n",
        "out_v = io.open('vectors.tsv', 'w', encoding='utf-8')\n",
        "out_m = io.open('metadata.tsv', 'w', encoding='utf-8')\n",
        "\n",
        "for index, word in enumerate(wordsInVocab[:1000]):\n",
        "  if index == 0:\n",
        "    continue  # skip 0, it's padding.\n",
        "  vec = embedWeights[index]\n",
        "  out_v.write('\\t'.join([str(x) for x in vec]) + \"\\n\")\n",
        "  out_m.write(word + \"\\n\")\n",
        "out_v.close()\n",
        "out_m.close()"
      ],
      "execution_count": 242,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d5K9jiqPA2n5"
      },
      "source": [
        "http://projector.tensorflow.org/ visualizing data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ED5FrFqgBx40"
      },
      "source": [
        "### Model 2: LSTM\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rLlBr6mm-NkZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "64d23f7b-287a-4a13-85d0-aa3c3108ad21"
      },
      "source": [
        "tf.random.set_seed(42)\n",
        "from tensorflow.keras import layers\n",
        "model2Embedding = layers.Embedding(input_dim=1000,\n",
        "                                     output_dim=128,\n",
        "                                     embeddings_initializer=\"uniform\",\n",
        "                                     input_length=avrageWord,\n",
        "                                     name=\"embedding_2\")\n",
        "\n",
        "\n",
        "# Create LSTM model\n",
        "inputs = layers.Input(shape=(1,), dtype=\"string\")\n",
        "x = textVector(inputs)\n",
        "x = model2Embedding(x)\n",
        "print(x.shape)\n",
        "# x = layers.LSTM(64, return_sequences=True)(x) # return vector for each word in the Tweet (you can stack RNN cells as long as return_sequences=True)\n",
        "x = layers.LSTM(64)(x) # return vector for whole sequence\n",
        "print(x.shape)\n",
        "# x = layers.Dense(64, activation=\"relu\")(x) # optional dense layer on top of output of LSTM cell\n",
        "outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
        "model2 = tf.keras.Model(inputs, outputs, name=\"model_2_LSTM\")"
      ],
      "execution_count": 243,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(None, None, 128)\n",
            "(None, 64)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RTsljA6FCdVz"
      },
      "source": [
        "\n",
        "# Compile model\n",
        "model2.compile(loss=\"binary_crossentropy\",\n",
        "                optimizer=tf.keras.optimizers.Adam(),\n",
        "                metrics=[\"accuracy\"])"
      ],
      "execution_count": 244,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qndlcYHpClkR",
        "outputId": "7537452c-7ff9-41c7-c8a5-95d5274bdace"
      },
      "source": [
        "model2History = model2.fit(xTrain,\n",
        "                              yTrain,\n",
        "                              epochs=5,\n",
        "                              validation_data=(xVal, yVal),\n",
        "                              callbacks=[CreateTensorboardCallback(SAVE_DIR, \n",
        "                                                                     \"LSTM\")])"
      ],
      "execution_count": 245,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving TensorBoard log files to: model_logs/LSTM/20211028-090551\n",
            "Epoch 1/5\n",
            "203/203 [==============================] - 6s 20ms/step - loss: 0.5602 - accuracy: 0.7061 - val_loss: 0.4632 - val_accuracy: 0.7925\n",
            "Epoch 2/5\n",
            "203/203 [==============================] - 3s 13ms/step - loss: 0.4246 - accuracy: 0.8193 - val_loss: 0.4651 - val_accuracy: 0.7820\n",
            "Epoch 3/5\n",
            "203/203 [==============================] - 3s 14ms/step - loss: 0.4009 - accuracy: 0.8346 - val_loss: 0.4967 - val_accuracy: 0.7723\n",
            "Epoch 4/5\n",
            "203/203 [==============================] - 3s 14ms/step - loss: 0.3839 - accuracy: 0.8425 - val_loss: 0.4793 - val_accuracy: 0.7837\n",
            "Epoch 5/5\n",
            "203/203 [==============================] - 3s 13ms/step - loss: 0.3673 - accuracy: 0.8492 - val_loss: 0.4841 - val_accuracy: 0.7688\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M1-gix4CCqyQ"
      },
      "source": [
        "yPred2 = model2.predict(xVal)\n",
        "yPred2 = tf.squeeze(tf.round(yPred2))\n",
        "resualt2 = CalculateResults(yVal,yPred2)\n"
      ],
      "execution_count": 246,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0xDTAZUcPd7I"
      },
      "source": [
        "### Model 3: GRU"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_K3d-baTC85B",
        "outputId": "19a10081-0346-46c9-f21b-0d12d5200c97"
      },
      "source": [
        "tf.random.set_seed(42)\n",
        "from tensorflow.keras import layers\n",
        "model3Embedding = layers.Embedding(input_dim=1000,\n",
        "                                     output_dim=128,\n",
        "                                     embeddings_initializer=\"uniform\",\n",
        "                                     input_length=avrageWord,\n",
        "                                     name=\"embedding_3\")\n",
        "\n",
        "\n",
        "# Create GRu model\n",
        "inputs = layers.Input(shape=(1,), dtype=\"string\")\n",
        "x = textVector(inputs)\n",
        "x = model3Embedding(x)\n",
        "print(x.shape)\n",
        "\n",
        "x = layers.GRU(64)(x) \n",
        "print(x.shape)\n",
        "\n",
        "outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
        "model3 = tf.keras.Model(inputs, outputs, name=\"model_3_GRU\")"
      ],
      "execution_count": 247,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(None, None, 128)\n",
            "(None, 64)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H0gG8vU4DMlw"
      },
      "source": [
        "# Compile GRU model\n",
        "model3.compile(loss=\"binary_crossentropy\",\n",
        "                optimizer=tf.keras.optimizers.Adam(),\n",
        "                metrics=[\"accuracy\"])"
      ],
      "execution_count": 248,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hJL9qf5QDPmC",
        "outputId": "219b3f80-7097-415f-ffc5-6bfce422ea00"
      },
      "source": [
        "model3History = model3.fit(xTrain,\n",
        "                              yTrain,\n",
        "                              epochs=5,\n",
        "                              validation_data=(xVal, yVal),\n",
        "                              callbacks=[CreateTensorboardCallback(SAVE_DIR, \n",
        "                                                                     \"GRU\")])"
      ],
      "execution_count": 249,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving TensorBoard log files to: model_logs/GRU/20211028-090615\n",
            "Epoch 1/5\n",
            "203/203 [==============================] - 6s 19ms/step - loss: 0.6608 - accuracy: 0.5999 - val_loss: 0.5142 - val_accuracy: 0.7706\n",
            "Epoch 2/5\n",
            "203/203 [==============================] - 3s 13ms/step - loss: 0.4599 - accuracy: 0.7979 - val_loss: 0.4609 - val_accuracy: 0.7925\n",
            "Epoch 3/5\n",
            "203/203 [==============================] - 3s 14ms/step - loss: 0.4078 - accuracy: 0.8292 - val_loss: 0.4937 - val_accuracy: 0.7811\n",
            "Epoch 4/5\n",
            "203/203 [==============================] - 3s 14ms/step - loss: 0.3885 - accuracy: 0.8397 - val_loss: 0.4795 - val_accuracy: 0.7837\n",
            "Epoch 5/5\n",
            "203/203 [==============================] - 3s 13ms/step - loss: 0.3730 - accuracy: 0.8516 - val_loss: 0.4839 - val_accuracy: 0.7785\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jPMhd2SSDXhB"
      },
      "source": [
        "yPred3 = model3.predict(xVal)\n",
        "yPred3 = tf.squeeze(tf.round(yPred3))\n",
        "resualt3 = CalculateResults(yVal,yPred3)\n"
      ],
      "execution_count": 250,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yiycDrJfLTE9"
      },
      "source": [
        "### Model 4: RNN bidirectional\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WVVxUCTSDe5T",
        "outputId": "d892bc8e-3db1-493f-adcc-2f9f7934cbc9"
      },
      "source": [
        "tf.random.set_seed(42)\n",
        "from tensorflow.keras import layers\n",
        "model4Embedding = layers.Embedding(input_dim=1000,\n",
        "                                     output_dim=128,\n",
        "                                     embeddings_initializer=\"uniform\",\n",
        "                                     input_length=avrageWord,\n",
        "                                     name=\"embedding_4\")\n",
        "\n",
        "\n",
        "# Create RNN model\n",
        "inputs = layers.Input(shape=(1,), dtype=\"string\")\n",
        "x = textVector(inputs)\n",
        "x = model4Embedding(x)\n",
        "print(x.shape)\n",
        "\n",
        "x = layers.Bidirectional(layers.LSTM(64))(x) # bidirectional goes both ways so has double the parameters of a regular LSTM layer\n",
        "outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
        "\n",
        "\n",
        "model4 = tf.keras.Model(inputs, outputs, name=\"model_4_RNN\")"
      ],
      "execution_count": 251,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(None, None, 128)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tlt9cAZ2D2XB"
      },
      "source": [
        "\n",
        "# Compile\n",
        "model4.compile(loss=\"binary_crossentropy\",\n",
        "                optimizer=tf.keras.optimizers.Adam(),\n",
        "                metrics=[\"accuracy\"])"
      ],
      "execution_count": 252,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hiib5RS0D4jC",
        "outputId": "6b285b1a-ce40-4f57-a78f-67025cbe6b1f"
      },
      "source": [
        "model4History = model4.fit(xTrain,\n",
        "                              yTrain,\n",
        "                              epochs=5,\n",
        "                              validation_data=(xVal, yVal),\n",
        "                              callbacks=[CreateTensorboardCallback(SAVE_DIR, \n",
        "                                                                     \"RNN\")])"
      ],
      "execution_count": 253,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving TensorBoard log files to: model_logs/RNN/20211028-090634\n",
            "Epoch 1/5\n",
            "203/203 [==============================] - 10s 31ms/step - loss: 0.5326 - accuracy: 0.7251 - val_loss: 0.4498 - val_accuracy: 0.7942\n",
            "Epoch 2/5\n",
            "203/203 [==============================] - 4s 21ms/step - loss: 0.4139 - accuracy: 0.8200 - val_loss: 0.4571 - val_accuracy: 0.7828\n",
            "Epoch 3/5\n",
            "203/203 [==============================] - 4s 21ms/step - loss: 0.3899 - accuracy: 0.8336 - val_loss: 0.4744 - val_accuracy: 0.7758\n",
            "Epoch 4/5\n",
            "203/203 [==============================] - 4s 21ms/step - loss: 0.3743 - accuracy: 0.8404 - val_loss: 0.4697 - val_accuracy: 0.7802\n",
            "Epoch 5/5\n",
            "203/203 [==============================] - 4s 21ms/step - loss: 0.3511 - accuracy: 0.8447 - val_loss: 0.4771 - val_accuracy: 0.7688\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rQYWTuBzD_IB"
      },
      "source": [
        "yPred4 = model4.predict(xVal)\n",
        "yPred4 = tf.squeeze(tf.round(yPred4))\n",
        "resualt4 = CalculateResults(yVal,yPred4)\n"
      ],
      "execution_count": 254,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hEGCrScaLgZf"
      },
      "source": [
        "### Model 5: Transfer Learning\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ysIDEvMaEKnW"
      },
      "source": [
        "# Example of pretrained embedding with universal sentence encoder - https://tfhub.dev/google/universal-sentence-encoder/4\n",
        "import tensorflow_hub as hub\n",
        "embed = hub.load(\"https://tfhub.dev/google/universal-sentence-encoder/4\") # load Universal Sentence Encoder\n"
      ],
      "execution_count": 255,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UVr5T7LnEVKu"
      },
      "source": [
        "# We can use this encoding layer in place of our text_vectorizer and embedding layer\n",
        "sentenceEncoderLayer = hub.KerasLayer(\"https://tfhub.dev/google/universal-sentence-encoder/4\",\n",
        "                                        input_shape=[], # shape of inputs coming to our model \n",
        "                                        dtype=tf.string, # data type of inputs coming to the USE layer\n",
        "                                        trainable=False, # keep the pretrained weights (we'll create a feature extractor)\n",
        "                                        name=\"USE\")"
      ],
      "execution_count": 256,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tsYjIkilEiBh"
      },
      "source": [
        "# Create model using the Sequential API\n",
        "model5 = tf.keras.Sequential([\n",
        "  sentenceEncoderLayer, # take in sentences and then encode them into an embedding\n",
        "  layers.Dense(64, activation=\"relu\"),\n",
        "  layers.Dense(1, activation=\"sigmoid\")\n",
        "], name=\"model_5_USE\")\n",
        "\n",
        "# Compile model\n",
        "model5.compile(loss=\"binary_crossentropy\",\n",
        "                optimizer=tf.keras.optimizers.Adam(),\n",
        "                metrics=[\"accuracy\"])"
      ],
      "execution_count": 257,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gZxhw--NEvJ1",
        "outputId": "3b755ce5-272c-487b-8cb7-5197b1c69b6d"
      },
      "source": [
        "model5History = model4.fit(xTrain,\n",
        "                              yTrain,\n",
        "                              epochs=5,\n",
        "                              validation_data=(xVal, yVal),\n",
        "                              callbacks=[CreateTensorboardCallback(SAVE_DIR, \n",
        "                                                                     \"RNN\")])"
      ],
      "execution_count": 258,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving TensorBoard log files to: model_logs/RNN/20211028-090731\n",
            "Epoch 1/5\n",
            "203/203 [==============================] - 6s 30ms/step - loss: 0.3277 - accuracy: 0.8642 - val_loss: 0.5336 - val_accuracy: 0.7609\n",
            "Epoch 2/5\n",
            "203/203 [==============================] - 4s 20ms/step - loss: 0.3070 - accuracy: 0.8717 - val_loss: 0.5238 - val_accuracy: 0.7688\n",
            "Epoch 3/5\n",
            "203/203 [==============================] - 4s 20ms/step - loss: 0.2820 - accuracy: 0.8841 - val_loss: 0.6026 - val_accuracy: 0.7539\n",
            "Epoch 4/5\n",
            "203/203 [==============================] - 4s 20ms/step - loss: 0.2688 - accuracy: 0.8838 - val_loss: 0.6040 - val_accuracy: 0.7426\n",
            "Epoch 5/5\n",
            "203/203 [==============================] - 4s 20ms/step - loss: 0.2426 - accuracy: 0.8983 - val_loss: 0.6063 - val_accuracy: 0.7434\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gE03nK2cExp0"
      },
      "source": [
        "yPred5 = model5.predict(xVal)\n",
        "yPred5 = tf.squeeze(tf.round(yPred5))\n",
        "resualt5 = CalculateResults(yVal,yPred5)\n"
      ],
      "execution_count": 259,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M7EO3vGHQCAl"
      },
      "source": [
        "### Compare resaults"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 234
        },
        "id": "iGAWo4k9E_b8",
        "outputId": "f2616af5-6c2a-48bd-9c8f-8f908b711fc0"
      },
      "source": [
        "\n",
        "# Combine model results into a DataFrame\n",
        "allModelRresults = pd.DataFrame({\"naive\": resualt0,\n",
        "                                  \"ANN\": resualt1,\n",
        "                                  \"lstm\": resualt2,\n",
        "                                  \"gru\": resualt3,\n",
        "                                  \"RNN\": resualt4,\n",
        "                                  \"transfer learning\": resualt5,\n",
        "})\n",
        "allModelRresults = allModelRresults.transpose()\n",
        "allModelRresults"
      ],
      "execution_count": 260,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>accuracy</th>\n",
              "      <th>precision</th>\n",
              "      <th>recall</th>\n",
              "      <th>f1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>naive</th>\n",
              "      <td>79.159370</td>\n",
              "      <td>0.801303</td>\n",
              "      <td>0.791594</td>\n",
              "      <td>0.785103</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>ANN</th>\n",
              "      <td>78.721541</td>\n",
              "      <td>0.786869</td>\n",
              "      <td>0.787215</td>\n",
              "      <td>0.785288</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>lstm</th>\n",
              "      <td>76.882662</td>\n",
              "      <td>0.768848</td>\n",
              "      <td>0.768827</td>\n",
              "      <td>0.765824</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>gru</th>\n",
              "      <td>77.845884</td>\n",
              "      <td>0.781198</td>\n",
              "      <td>0.778459</td>\n",
              "      <td>0.774090</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>RNN</th>\n",
              "      <td>76.882662</td>\n",
              "      <td>0.768026</td>\n",
              "      <td>0.768827</td>\n",
              "      <td>0.766782</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>transfer learning</th>\n",
              "      <td>47.635727</td>\n",
              "      <td>0.563600</td>\n",
              "      <td>0.476357</td>\n",
              "      <td>0.411437</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                    accuracy  precision    recall        f1\n",
              "naive              79.159370  0.801303   0.791594  0.785103\n",
              "ANN                78.721541  0.786869   0.787215  0.785288\n",
              "lstm               76.882662  0.768848   0.768827  0.765824\n",
              "gru                77.845884  0.781198   0.778459  0.774090\n",
              "RNN                76.882662  0.768026   0.768827  0.766782\n",
              "transfer learning  47.635727  0.563600   0.476357  0.411437"
            ]
          },
          "metadata": {},
          "execution_count": 260
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n4_3S6O5zYvg"
      },
      "source": [
        "### Remove Punctaution"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qN3zvhsb56sl"
      },
      "source": [
        "import string \n",
        "\n",
        "# Create a function for removing punctuation\n",
        "def RemovePunctaution(text):\n",
        "    noPunctaution = [words for words in text if words not in string.punctuation]\n",
        "    return ''.join(noPunctaution)"
      ],
      "execution_count": 275,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SSocQARGQluO"
      },
      "source": [
        "# Data processing and modeling with best of models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iiQsLbo8yLsx"
      },
      "source": [
        "### Convert uppercsae to lowercase"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ckFrzrx78nFp"
      },
      "source": [
        "\n",
        "# Make a fucntion for convert Upper letter to lower letter\n",
        "def UpperToLower(text):\n",
        "    noUpperCase = [words.lower() for words in text]\n",
        "    return ''.join(noUpperCase)\n"
      ],
      "execution_count": 262,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_Wd2S66Q8zLg",
        "outputId": "988cd537-54d2-4856-c6a6-df1d686bc73c"
      },
      "source": [
        "import nltk\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "# Testing nltk \n",
        "stopWords = nltk.corpus.stopwords.words('english')\n",
        "len(stopWords)"
      ],
      "execution_count": 263,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "179"
            ]
          },
          "metadata": {},
          "execution_count": 263
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gIZ4BOHbyVyc"
      },
      "source": [
        "### Stopwords"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Xf4BF6u_Y-A"
      },
      "source": [
        "\n",
        "# Create a Function for removing stop word\n",
        "\n",
        "def RemoveStopWords(text):\n",
        "  stopWords = nltk.corpus.stopwords.words('english')\n",
        "  noStopWords = [word for word in text if word not in stopWords]\n",
        "  return noStopWords\n"
      ],
      "execution_count": 264,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GhSNufTiNFFy"
      },
      "source": [
        "### Tokenizing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bwY_Wqm7NHhf"
      },
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "def Tokening(string):\n",
        "  tokens = word_tokenize(string)\n",
        "  return tokens"
      ],
      "execution_count": 301,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-8YkB6re_3mY"
      },
      "source": [
        "## Spell Correction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yam1JtmBCAqH",
        "outputId": "0552840c-078c-4119-bde3-4b54ff9cb894"
      },
      "source": [
        "! python -m pip install -U symspellpy"
      ],
      "execution_count": 266,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: symspellpy in /usr/local/lib/python3.7/dist-packages (6.7.0)\n",
            "Requirement already satisfied: numpy>=1.13.1 in /usr/local/lib/python3.7/dist-packages (from symspellpy) (1.19.5)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "muP1sPzvCU1D",
        "outputId": "e3fa7a09-b600-4798-81a4-5ec085879d29"
      },
      "source": [
        "!curl -LJO https://raw.githubusercontent.com/mammothb/symspellpy/master/symspellpy/frequency_dictionary_en_82_765.txt\n",
        "!curl -LJO https://raw.githubusercontent.com/mammothb/symspellpy/master/symspellpy/frequency_bigramdictionary_en_243_342.txt"
      ],
      "execution_count": 267,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100 1300k  100 1300k    0     0  2762k      0 --:--:-- --:--:-- --:--:-- 2762k\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100 5016k  100 5016k    0     0  6239k      0 --:--:-- --:--:-- --:--:-- 6232k\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qu-IvV8G_2B3"
      },
      "source": [
        "from symspellpy import SymSpell, Verbosity\n",
        "\n",
        "symSpell = SymSpell()\n",
        "PATH = \"./frequency_dictionary_en_82_765.txt\"\n",
        "symSpell.load_dictionary(PATH,0,1)\n",
        "def SpellingCorrection(sent):\n",
        "  correctSpelling = []\n",
        "  for word in sent.split(\" \"):\n",
        "    x = symSpell.lookup(word,Verbosity.CLOSEST, max_edit_distance=2,include_unknown=True)[0].__str__()\n",
        "    y = x.split(',')[0]\n",
        "    correctSpelling.append(y)\n",
        "  return correctSpelling\n"
      ],
      "execution_count": 279,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HN1OvGBHI0CL",
        "outputId": "7f34fa94-457c-4373-87d3-f54ca7f6b484"
      },
      "source": [
        "string = ' hello my nam is joj wha is yo name and what ar yo doin'\n",
        "SpellingCorrection(string)\n"
      ],
      "execution_count": 269,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "a, 1, 9081174698\n",
            "hello, 0, 32960381\n",
            "my, 0, 1059793441\n",
            "nam, 0, 7037733\n",
            "is, 0, 4705743816\n",
            "job, 1, 177706929\n",
            "what, 1, 812395582\n",
            "is, 0, 4705743816\n",
            "to, 1, 12136980858\n",
            "name, 0, 464532702\n",
            "and, 0, 12997637966\n",
            "what, 0, 812395582\n",
            "a, 1, 9081174698\n",
            "to, 1, 12136980858\n",
            "down, 1, 224915894\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['a',\n",
              " 'hello',\n",
              " 'my',\n",
              " 'nam',\n",
              " 'is',\n",
              " 'job',\n",
              " 'what',\n",
              " 'is',\n",
              " 'to',\n",
              " 'name',\n",
              " 'and',\n",
              " 'what',\n",
              " 'a',\n",
              " 'to',\n",
              " 'down']"
            ]
          },
          "metadata": {},
          "execution_count": 269
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wiljhNAVINn3"
      },
      "source": [
        "from symspellpy import SymSpell, Verbosity\n",
        "\n",
        "symSpell = SymSpell()\n",
        "PATH = \"./frequency_dictionary_en_82_765.txt\"\n",
        "symSpell.load_dictionary(PATH,0,1)\n",
        "def SpellingRemover(sent):\n",
        "  correctSpelling = []\n",
        "  for word in sent.split(\" \"):\n",
        "    x = symSpell.lookup(word,Verbosity.CLOSEST, max_edit_distance=2,include_unknown=True)[0].__str__()\n",
        "    if x.split(',')[1]==' 0':\n",
        "      correctSpelling.append(word)\n",
        "\n",
        "  return correctSpelling\n"
      ],
      "execution_count": 280,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nh1dau6Gyhkt"
      },
      "source": [
        "### Lemmatization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YagnyRPpUwT1"
      },
      "source": [
        "import nltk\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "def Lemmatizer(vector):\n",
        "  lemmatizer = WordNetLemmatizer()\n",
        "  lemmatizeWords = [lemmatizer.lemmatize(word) for word in vector]\n",
        "  return lemmatizeWords"
      ],
      "execution_count": 281,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C3MhbmXUysDf"
      },
      "source": [
        "### Convert to text"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jUtXOKJQhVwM"
      },
      "source": [
        "def ConvertArrayToText(vector):\n",
        "  return ' '.join(vector)"
      ],
      "execution_count": 282,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t3JGSbAXynbe"
      },
      "source": [
        "### Make pipline"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CmtEcl2xYd6z"
      },
      "source": [
        "import re\n",
        "def Pipline(data):\n",
        "  data['text-after-cleaning'] = data['text'].apply(lambda x : re.sub(r'@\\w+','',x))\n",
        "  data['text-after-cleaning'] = data['text-after-cleaning'].apply(lambda x :re.sub(r\"http\\S+\", '', x))\n",
        "  data['text-after-cleaning'] = data['text-after-cleaning'].apply(lambda x :re.sub(r\"[0-9]+\", '', x))\n",
        "  data['text-after-cleaning'] = data['text-after-cleaning'].apply(lambda x : RemovePunctaution(x))\n",
        "  data['text-after-cleaning'] = data['text-after-cleaning'].apply(lambda x : UpperToLower(x))\n",
        "  # Dosen't work very well\n",
        "  # data['text-after-cleaning'] = data['text-after-cleaning'].apply(lambda x : SpellingRemover(x))\n",
        "  data['text-after-cleaning'] = data['text-after-cleaning'].apply(lambda x : Tokening(x))\n",
        "  data['text-after-cleaning'] = data['text-after-cleaning'].apply(lambda x : RemoveStopWords(x))\n",
        "  data['text-after-cleaning'] = data['text-after-cleaning'].apply(lambda x : Lemmatizer(x))\n",
        "  data['text-after-cleaning'] = data['text-after-cleaning'].apply(lambda x : ConvertArrayToText(x))\n",
        "\n",
        " \n"
      ],
      "execution_count": 302,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LsDOvXAHZD1H"
      },
      "source": [
        "Pipline(train_data)\n",
        "Pipline(xTest)\n"
      ],
      "execution_count": 303,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "crr5t8EMKjXw",
        "outputId": "d40e7488-47c3-4cdc-9c07-8a67a97bac4f"
      },
      "source": [
        "import random\n",
        "randomIndex = random.randint(0, len(train_data)-5) \n",
        "for row in train_data[[\"text-after-cleaning\", \"target\"]][randomIndex:randomIndex+5].itertuples():\n",
        "  _, text, target = row\n",
        "  print(f'Original: {train_data.text.iloc[[row[0]]]}')\n",
        "  print(f\"row : {randomIndex} Target: {target}\", \"(real disaster)\" if target > 0 else \"(not real disaster)\")\n",
        "  print(f\"Text:\\n{text}\\n\")\n",
        "  print(\"---\\n\")"
      ],
      "execution_count": 304,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original: 6466    @aphyr IÛªve been following you this longÛ_ Sunk cost fallacy or somethinÛª\n",
            "Name: text, dtype: object\n",
            "row : 6466 Target: 0 (not real disaster)\n",
            "Text:\n",
            "iûªve following longû sunk cost fallacy somethinûª\n",
            "\n",
            "---\n",
            "\n",
            "Original: 6467    descended or sunk however it may be to the shadowed land beyond the crest of a striking cobra landing harshly upon his back; torch and\n",
            "Name: text, dtype: object\n",
            "row : 6466 Target: 0 (not real disaster)\n",
            "Text:\n",
            "descended sunk however may shadowed land beyond crest striking cobra landing harshly upon back torch\n",
            "\n",
            "---\n",
            "\n",
            "Original: 6468    So if I capsize on your thighs high tide B-5 you sunk my battleship\\n&gt;\n",
            "Name: text, dtype: object\n",
            "row : 6466 Target: 0 (not real disaster)\n",
            "Text:\n",
            "capsize thigh high tide b sunk battleship gt\n",
            "\n",
            "---\n",
            "\n",
            "Original: 6469    has NOT sunk in that i leave for school in a month\n",
            "Name: text, dtype: object\n",
            "row : 6466 Target: 0 (not real disaster)\n",
            "Text:\n",
            "sunk leave school month\n",
            "\n",
            "---\n",
            "\n",
            "Original: 6470    It still hasn't sunk in that i will never see my nan again how has it been 2 months already ??\n",
            "Name: text, dtype: object\n",
            "row : 6466 Target: 0 (not real disaster)\n",
            "Text:\n",
            "still hasnt sunk never see nan month already\n",
            "\n",
            "---\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nVyuW7aAy1yq"
      },
      "source": [
        "### Use best model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ubfyi9SUaw-Q"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "xTrain = train_data['text-after-cleaning']\n",
        "yTrain = train_data['target']\n",
        "\n",
        "id = xTest['id']"
      ],
      "execution_count": 314,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8w_e72eTe1c1",
        "outputId": "a456de69-781d-4233-a2d4-f5c52e387bd1"
      },
      "source": [
        "# reuse best model \n",
        "model0.fit(xTrain, yTrain)"
      ],
      "execution_count": 315,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Pipeline(memory=None,\n",
              "         steps=[('tfidf',\n",
              "                 TfidfVectorizer(analyzer='word', binary=False,\n",
              "                                 decode_error='strict',\n",
              "                                 dtype=<class 'numpy.float64'>,\n",
              "                                 encoding='utf-8', input='content',\n",
              "                                 lowercase=True, max_df=1.0, max_features=None,\n",
              "                                 min_df=1, ngram_range=(1, 1), norm='l2',\n",
              "                                 preprocessor=None, smooth_idf=True,\n",
              "                                 stop_words=None, strip_accents=None,\n",
              "                                 sublinear_tf=False,\n",
              "                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
              "                                 tokenizer=None, use_idf=True,\n",
              "                                 vocabulary=None)),\n",
              "                ('clf',\n",
              "                 MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))],\n",
              "         verbose=False)"
            ]
          },
          "metadata": {},
          "execution_count": 315
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iS4GCngCibwx"
      },
      "source": [
        "resualt = model0.predict(xTest['text-after-cleaning'])\n",
        "resualt = tf.squeeze(tf.round(resualt))\n",
        "resaultDataframe = pd.DataFrame({'id':id,'target':resualt})\n",
        "resaultDataframe.to_csv('submission.csv',index=False)"
      ],
      "execution_count": 316,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q8z1lC8nSYAg"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}